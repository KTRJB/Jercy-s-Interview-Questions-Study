## 1. **메모리의 개념과 역할**

### 컴퓨터의 저장 개념

컴퓨터에서의 저장에는 크게 두 가지가 있습니다:

1. **오래 저장하는 것**: 파일 형태로 하드디스크와 같은 디스크에 저장됩니다. 전원을 꺼도 데이터가 유지되며, 주로 대용량 데이터를 장기간 저장하는 데 사용됩니다.
2. **잠깐 저장하는 것**: 주로 CPU가 필요로 하는 명령어와 데이터를 **메모리**에 저장합니다. 이 메모리는 휘발성이라 전원을 끄면 데이터가 사라집니다. 이는 컴퓨터의 성능을 높이기 위한 임시 기억 창고 역할을 합니다.


### 메모리란?

- 메모리는 컴퓨터에서 프로그램 실행과 데이터 처리를 위해 데이터를 저장하는 장치입니다.
- 메모리에는 여러 종류가 있으며, CPU가 프로그램을 실행할 때 데이터와 명령어를 일시적으로 저장하여 빠르게 접근할 수 있게 해줍니다.


### 메모리가 중요한 이유

- CPU가 처리할 명령어와 데이터는 하드디스크에 있는 파일에서 가져옵니다. 
- 하지만 하드디스크에서 데이터를 바로 읽는 것은 느리기 때문에, 파일의 일부를 메모리에 옮겨 CPU가 빠르게 접근할 수 있도록 합니다. 
- 즉, 메모리는 CPU가 필요한 데이터를 바로 제공해 컴퓨터 성능을 높여줍니다.


---

## 2. **메모리의 동작 원리**

### 메모리의 주소 지정 방식

메모리는 행과 열로 이루어진 **매트릭스 구조**로 데이터를 저장하고 관리합니다. 이를 통해 데이터를 찾고 저장하는 과정이 최적화됩니다. 각각의 메모리 셀은 **특정한 행(Row)과 열(Column)의 교차점**에 위치하며, CPU가 데이터를 요청하면 행과 열 주소를 통해 메모리 셀에 접근하게 됩니다.

1. **RAS (Row Address Strobe)**: 행 주소를 지정해 해당 행을 활성화합니다.
2. **CAS (Column Address Strobe)**: 열 주소를 지정해 데이터가 저장된 특정 열을 찾습니다.

이 두 신호를 통해 CPU가 특정 메모리 위치의 데이터를 읽거나 쓸 수 있게 됩니다.


### 메모리의 동작 원리

메모리의 동작 과정은 데이터를 효율적으로 접근하기 위한 일련의 사이클로 이루어져 있습니다.

1. **CPU가 데이터 요청**: CPU가 메인보드 칩셋에 데이터를 요청합니다. 칩셋은 요청된 데이터가 저장된 메모리 위치의 행 주소를 찾아 메모리로 보냅니다. 이 과정은 1 사이클이 소요됩니다.
    
2. **행 주소(RAS) 읽기**: 메모리 행 주소 버퍼가 요청된 행 주소를 받고, **센스 앰프(Sense Amp)**가 해당 행의 데이터를 읽어 냅니다. 이 과정을 **행 주소 스트로브(RAS)**라고 하며, 약 2~3 사이클이 소요됩니다.
    
3. **열 주소(CAS) 읽기**: 이어서 열 주소가 메모리에 전달됩니다. CAS 신호를 통해 열 주소를 찾아 데이터를 정확하게 접근합니다. 이 과정을 **열 주소 스트로브(CAS)**라고 하며, 이 역시 약 2~3 사이클이 소요됩니다.
    
4. **데이터 이동**: 행과 열 주소로 정확한 메모리 셀을 찾아 데이터를 읽은 후, 메모리 셀의 데이터가 **출력 버퍼(Output Buffer)**로 이동합니다. 이 과정에는 1 사이클이 걸립니다.
    
5. **CPU로 데이터 전달**: 메인보드 칩셋이 출력 버퍼의 데이터를 읽어 CPU로 전달합니다. 이 과정에도 약 2 사이클이 소요됩니다.


---

## 3. **메모리의 성능과 분류**

### 메모리 성능 관련 개념

1. 엑세스 타임(Access Time): CPU가 특정 주소의 데이터를 요청하고, 해당 데이터를 읽기 시작하기까지 걸리는 시간을 말합니다. 이 시간은 CPU에서 RAS와 CAS 신호를 보내고, 해당 위치에서 데이터를 읽어오는 데 걸리는 시간을 포함합니다.
    
2. **리프레시 타임**: DRAM과 같은 동적 메모리는 주기적으로 데이터를 재충전해야 합니다. 이 주기를 **리프레시 타임이라 하며, 리프레시가 제대로 이루어지지 않으면 데이터가 소멸됩니다.
    
3. **사이클 타임**: 메모리 작업이 완료된 후 다음 작업을 준비하는 데 걸리는 시간입니다. **사이클 시간 = 메모리 액세스 시간 + 리프레시 시간**으로 정의되며, 이 값이 짧을수록 메모리의 작업 처리 속도가 빨라집니다.


### 메모리와 성능의 관계

- CPU가 처리 속도가 아무리 빨라도 데이터를 메모리에서 가져오는 속도가 느리다면 성능이 저하됩니다.
- 이러한 속도 차이를 해결하기 위해 메모리 계층 구조(Hierarchy)를 사용하며, 주 기억 장치인 RAM과 그보다 빠른 캐시 메모리를 CPU에 추가하여 성능 저하를 막습니다.
- **CPU와 메모리 간의 속도 차이**를 해결하는 것이 컴퓨터 성능 향상의 핵심입니다.


### 메모리 계층 구조

메모리 계층 구조에서 **아래쪽 계층으로 내려갈수록** 다음과 같은 특성들이 나타납니다:

1. **비트당 비용 감소**
    - 메모리 계층 구조에서 아래쪽으로 갈수록 **비트당 저장 비용이 저렴**해집니다. 즉, 상위 계층의 메모리는 고속이고 고비용인 반면, 하위 계층의 메모리는 상대적으로 **저속이고 저비용**입니다.
2. **용량 증가**
    - 계층 구조의 하위에 위치한 메모리들은 **더 큰 용량**을 가질 수 있습니다. 상위 계층의 빠른 메모리보다 **더 많은 데이터를 저장**할 수 있습니다.
3. **접근 시간 증가 (속도 느려짐)**
    - 계층 구조에서 **아래쪽으로 갈수록 데이터 접근 시간이 증가**합니다. 즉, 하위 계층의 메모리는 **더 느리게 동작**하며, 상위 계층에 비해 데이터를 읽고 쓰는 데 시간이 더 걸립니다.
4. **처리기에 의한 메모리 접근 회수 감소**
    - 상위 계층의 **빠른 메모리**를 통해 CPU가 **더 빠르게 데이터에 접근**할 수 있어, 전체적으로 **메모리 접근 횟수가 감소**하고, 이를 통해 시스템의 **성능이 향상**됩니다.

![image](https://github.com/user-attachments/assets/250f383e-d42b-4a9f-8b24-46b3adf207af)
(출처: https://velog.io/@ajm0718/%EB%A9%94%EB%AA%A8%EB%A6%AC-%EA%B5%AC%EC%A1%B0)

![image](https://github.com/user-attachments/assets/6ea8be4b-ae95-4930-81f1-178d11c6bf27)
(출처: https://iingang.github.io/posts/OS-memory/)


### 메모리의 분류

1. **RAM**: 주기억 장치로 사용되며, 자료 저장 방식에 따라 **SRAM**(정적 RAM)과 **DRAM**(동적 RAM)으로 나뉩니다.
    - **SRAM**: 전력 공급만으로 데이터를 유지하며, 주기적인 재충전이 필요하지 않아 속도가 빠릅니다.
    - **DRAM**: 주기적으로 데이터를 재충전해야 하며, 상대적으로 느립니다.
2. **ROM**: 데이터를 영구히 저장하는 비휘발성 메모리입니다. EPROM, EEPROM 등으로 나뉘며, 저장된 데이터를 전기적 신호로 수정할 수 있습니다.
    
3. **캐시 메모리**: CPU와 메모리 사이에서 데이터 전송 속도를 높이기 위해 사용됩니다. 자주 사용하는 데이터를 미리 저장하여 빠른 접근을 가능하게 하며, CPU의 성능을 높이는 중요한 역할을 합니다.
    
4. **보조 기억 장치**: 하드디스크, SSD, USB와 같은 장치로, 데이터를 영구히 저장하는 역할을 합니다. 메모리에 비해 속도가 느리지만 큰 용량의 데이터를 저장할 수 있습니다.


---

## 4. **캐시 메모리의 개념과 역할**

### 캐시 메모리란?

- 캐시 메모리는 **CPU에 내장된 고속 메모리**로, 자주 사용하는 데이터를 **임시 저장**해 CPU가 더 빠르게 접근할 수 있도록 합니다.
- 캐시는 메모리와 CPU의 속도 차이를 극복하기 위한 해결책으로, 주 기억 장치에서 데이터를 불러오는 시간이 오래 걸리는 경우를 대비해 필요한 데이터를 미리 준비해둡니다.


### 캐시 메모리의 역할

1. **CPU의 대기 시간 최소화**: CPU는 데이터가 메모리에서 도착하기를 기다리는 동안 작업이 중단됩니다. 캐시가 이를 완화하여 **CPU의 효율적인 작동**을 돕습니다.
    
2. **데이터 접근 시간 단축**: 캐시는 CPU가 필요로 하는 데이터를 미리 저장해두고, 자주 사용하는 데이터는 캐시에서 가져오도록 하여 메모리 접근 시간을 크게 줄입니다.
    
3. **성능 최적화**: 캐시 히트(Cache Hit) 시에는 주 기억 장치에 접근할 필요가 없어, 컴퓨터의 성능이 최적화됩니다.


### 캐시 메모리 레벨

캐시 메모리는 CPU 내 여러 레벨로 나뉘며, **속도**와 **접근 방법**에 따라 L1, L2, L3로 구분됩니다:

1. **L1 캐시**: CPU 코어마다 따로 **내장**된 가장 빠르고 작은 캐시로, 코어별로 독립적으로 사용되는 private cache 입니다.
    
2. **L2 캐시**: L1보다 크지만 다소 느린 캐시로, 코어별로 존재하기도 하고 공유 형태(shared cache)로 존재하기도 합니다. 캐시에서 데이터를 찾지 못할 경우 L1에서 L2로 데이터를 확인합니다.
    
3. **L3 캐시**: L2보다 크고 다소 느리지만, 여러 코어가 공유하는 캐시입니다. L3 캐시가 추가되어 여러 코어가 공유할 수 있는 큰 캐시 영역을 제공하며, 이를 통해 데이터 접근 속도를 더 높입니다.


---

## 5. **캐시 메모리 작동 원리와 성능**

### 작동 원리

- CPU가 메모리에 있는 데이터를 요청하면, 먼저 캐시에서 해당 데이터가 있는지 확인합니다. 
- 데이터가 캐시에 있을 경우 **히트(Hit)**라고 하며, 데이터를 캐시에서 바로 가져옵니다. 
- 히트가 발생할 확률은 **히트율(Hit Rate)**로 측정됩니다. 
- 반면 데이터가 없으면 **미스(Miss)**가 발생하고, 메모리에서 데이터를 불러와 캐시에 저장합니다.


### 캐시 미스 종류

- cold miss : 메모리 주소가 처음 불려서 발생.
- conflict miss : 다른 데이터와 같은 캐시 주소에 할당되어 충돌 발생.
- capacity miss : 캐시 용량 부족으로 발생.


### 캐시 메모리의 성능 지표

1. **히트율(Hit Rate): CPU가 요청한 데이터 중 캐시에 저장된 비율
    
2. **히트 타임(Hit Time)**: 캐시에서 데이터를 읽어오는 데 걸리는 시간
    
3. **미스 패널티(Miss Penalty)**: 메모리에서 캐시로 가져오는 데 걸리는 시간


### 캐시 메모리 성능 개선 요소

- **캐시 크기**: 캐시 크기가 클수록 히트율이 높아질 가능성이 큽니다.
- **인출 방식**: 데이터 필요 시 가져오는 **요구 인출**과 데이터를 미리 가져오는 **선 인출** 방식이 있습니다.
- **쓰기 정책**: Write Through와 Write Back 정책 중 선택합니다.
- **교체 알고리즘**: FIFO, LRU 등 교체 알고리즘을 사용해 교체 효율을 높입니다.


### 인출 방식(Fetch Algorithm)


**인출 방식**(Fetch Algorithm)은 캐시가 데이터를 메인 메모리에서 가져오는 시점을 결정하는 방식입니다. 주로 **요구 인출**과 **선 인출** 방식이 사용됩니다.

1. **요구 인출 (Demand Fetch)**
    - **정의**: CPU가 특정 데이터를 필요로 할 때, 그 데이터를 캐시에 불러오는 방식입니다.
    - **장점**: 필요할 때만 메모리 접근이 이루어져 메모리 대역폭을 효율적으로 사용합니다.
    - **단점**: 데이터를 즉시 사용하지 못해 **미스 패널티**가 발생할 수 있습니다.
2. **선 인출 (Pre-Fetch)**
    - **정의**: CPU가 데이터가 필요하기 전에, 미리 데이터를 예측하여 캐시에 불러오는 방식입니다.
    - **장점**: 필요한 데이터가 캐시에 있어 미스가 줄어들고, 성능 향상에 도움이 될 수 있습니다.
    - **단점**: 예측이 잘못되면 캐시 메모리가 불필요하게 차지되어, 오히려 성능 저하를 일으킬 수 있습니다.

선 인출 방식은 캐시 히트율을 높여 성능을 개선할 수 있지만, 과도한 선 인출은 캐시 오버헤드를 유발할 수 있어 **예측 정확도**가 중요합니다.


### 캐시 메모리의 쓰기 정책

- **Write Through**: CPU가 데이터를 캐시에 쓰는 즉시 메모리에도 반영하여 일관성을 유지합니다. 접근 시간은 느리지만 구조가 단순합니다.
- **Write Back**: 변경을 캐시에만 반영하고, 이후 해당 블록이 캐시에서 제거될 때 메모리에 기록하여 성능을 향상합니다.


### 캐시 메모리의 주소 매핑 방식

캐시는 메모리보다 작기 때문에 별도의 매핑 방식을 통해 메모리 주소를 캐시에 대응합니다. 주요 매핑 방식은 다음과 같습니다.

- **Direct Mapping**
	- 메모리 블록이 고정된 캐시 블록에 매핑됩니다. 구현이 간단하지만 동일한 캐시 블록이 반복 액세스되면 히트율이 낮아집니다.
	- ![image](https://github.com/user-attachments/assets/e3733e06-8c82-4c26-b486-8a7ed0adaf8e)
	- (출처: https://velog.io/@letskuku/%EC%BB%B4%ED%93%A8%ED%84%B0%EA%B5%AC%EC%A1%B0-%EC%BA%90%EC%8B%9C-%EB%A9%94%EB%AA%A8%EB%A6%AC)
- **Associative Mapping**
	- 메모리 블록이 캐시 내 모든 블록에 저장될 수 있습니다. **Direct Mapping**의 반대 개념으로, 높은 유연성을 제공하지만 hit인지 miss인지 판단하기 위해 cache의 모든 block을 확인해야한다
	- ![image](https://github.com/user-attachments/assets/8153d428-6a69-4dbd-92f0-236ff02eec74)
	- (출처: https://velog.io/@letskuku/%EC%BB%B4%ED%93%A8%ED%84%B0%EA%B5%AC%EC%A1%B0-%EC%BA%90%EC%8B%9C-%EB%A9%94%EB%AA%A8%EB%A6%AC)
- **Set Associative Mapping**
	- Direct Mapping과 Associative Mapping의 절충안으로, 캐시를 여러 세트로 나누고 메모리 블록을 각 세트 내 블록 중 하나에 저장합니다. 일반적으로 2-way, 4-way 등으로 구현됩니다.
	- ![image](https://github.com/user-attachments/assets/ab55fca3-a32e-49df-aba2-f6698087f718)
	- (출처: https://velog.io/@letskuku/%EC%BB%B4%ED%93%A8%ED%84%B0%EA%B5%AC%EC%A1%B0-%EC%BA%90%EC%8B%9C-%EB%A9%94%EB%AA%A8%EB%A6%AC)


### Direct Mapped Cache의 구조와 원리

1. **Direct Mapped Cache 개념**
    - **Direct Mapped Cache**는 각 **메모리 블록**이 캐시의 **한 블록**에만 저장될 수 있는 구조입니다.
    - 예를 들어, 메모리의 블록 주소가 `00001`이라면, 이 데이터는 캐시 메모리의 `001` 위치에만 저장될 수 있습니다.
    - 따라서 CPU가 특정 메모리 블록을 요청할 때, 그 데이터가 캐시에 있는지 확인하려면 오직 하나의 캐시 블록만 검사하면 됩니다.
2. **캐시 블록 주소 계산**
    - 메모리 블록을 캐시의 어느 블록에 저장할지 계산하기 위해 **주소를 나머지 연산**을 이용해 매핑합니다.
    - **메모리 블록 주소 % 캐시 블록 수** 공식을 통해 각 메모리 블록이 저장될 캐시 블록의 주소를 계산합니다.
    - 예를 들어, 메모리 블록 주소가 `00000`부터 `11111`(31)까지 있고 캐시가 8개 블록을 가지고 있다면, 메모리 블록 주소를 8로 나눈 나머지로 캐시 블록을 결정합니다.
        - 이렇게 하면, 메모리 블록 주소의 **하위 3비트**가 캐시 블록 주소가 됩니다.
3. **태그 (Tag) 활용**
    - ![image](https://github.com/user-attachments/assets/ceaa0e56-7d52-423a-8b84-d3491ea0ba1c)
    - (출처: https://velog.io/@letskuku/%EC%BB%B4%ED%93%A8%ED%84%B0%EA%B5%AC%EC%A1%B0-%EC%BA%90%EC%8B%9C-%EB%A9%94%EB%AA%A8%EB%A6%AC)
    - 캐시는 메모리보다 작기 때문에 여러 개의 메모리 블록이 **하나의 캐시 블록을 공유**하게 됩니다.
    - 따라서, 캐시의 각 블록이 어느 메모리 블록의 데이터인지 표시할 필요가 있습니다.
    - 이를 위해 **태그(Tag)** 필드를 사용하여, 현재 캐시 블록에 저장된 메모리 블록의 주소 상위 비트를 기록합니다.
    - 예를 들어, 캐시의 회색 부분(001)에 저장된 데이터가 메모리의 네 부분 중 어느 블록에 해당하는지(`00001`, `01001`, `10001`, `11001`) 태그를 통해 구분할 수 있습니다.
4. **구성 요소**
    - **Valid Bit**: 데이터가 유효한지 여부를 나타냅니다. 캐시가 초기화되면 0으로 설정되고, 유효한 데이터가 들어오면 1로 변경됩니다.
    - **Tag**: 메모리 블록이 캐시에 매핑될 때, 어느 메모리 블록이 저장되어 있는지를 나타냅니다.
    - **Data**: 캐시가 실제로 저장하는 데이터입니다.


### 캐시 메모리의 교체 알고리즘

캐시에 새로운 데이터를 저장할 공간이 없을 때 교체할 블록을 결정하는 **교체 알고리즘**은 캐시 성능에 큰 영향을 미칩니다.

- **FIFO**: 가장 오래된 블록을 교체합니다.
- **LRU (Least Recently Used)**: 최근에 사용되지 않은 블록을 교체합니다.
- **LFU (Least Frequently Used)**: 사용 빈도가 낮은 블록을 교체합니다.
- **Random**: 임의의 블록을 교체합니다.
- **Optimal**: 향후 가장 참조되지 않을 블록을 교체하는 방식이나, 실제 구현은 불가능합니다.


### 캐시 일관성 문제와 해결 방식

멀티프로세서 환경에서는 캐시에 저장된 데이터가 서로 다른 프로세서에 의해 불일치가 발생할 수 있습니다.

- **공유 캐시 사용**: 모든 프로세스가 하나의 캐시를 공유하여 일관성을 유지하지만 충돌이 빈번합니다.
- **버스 감시 메커니즘**: 캐시 상태를 감시해 변경 사항을 동기화하는 방식입니다.
- **디렉토리 기반 일관성 유지**: 중앙 집중식 디렉토리를 통해 데이터 일관성을 유지합니다.


---

## 6. **캐시의 지역성(Locality) 원리**

### 지역성(Locality)이란?

- 지역성(Locality)은 **프로그램이 실행되는 동안 데이터 접근이 특정 패턴을 따른다는 원리**입니다. CPU가 특정 데이터나 명령어를 한 번 사용할 때, 그 주변의 데이터나 명령어를 짧은 시간 내에 다시 사용할 가능성이 높다는 것을 의미합니다.
- 지역성의 원리는 캐시 메모리가 자주 사용하는 데이터를 효과적으로 저장하여 성능을 높이는 근거가 됩니다.


### 지역성의 종류

1. **시간 지역성(Temporal Locality)**:
    - 한 번 접근한 데이터나 명령어는 가까운 시점에 다시 접근할 가능성이 높다는 개념입니다.
    - 예를 들어, 반복문(for, while) 안에서 사용되는 변수는 계속해서 접근되므로, 한 번 캐시에 저장되면 성능이 크게 향상됩니다.
2. **공간 지역성(Spatial Locality)**:
    - 특정 메모리 위치에 접근하면, 그 주변 위치도 곧 접근할 가능성이 높다는 개념입니다.
    - 예를 들어, 배열의 요소들은 연속적으로 저장되어 있고 반복문으로 처리할 때 순차적으로 접근되므로, 배열의 첫 번째 요소를 캐시에 올리면 그 주변 요소들에 접근할 때 빠르게 처리할 수 있습니다.


### 지역성 원리에 따른 성능 최적화

- 지역성 원리를 적용하여 캐시는 자주 사용하는 데이터를 미리 저장해 둠으로써 **주 기억 장치 접근 횟수를 줄이고**, CPU가 데이터를 빠르게 가져올 수 있게 합니다. 
- 이로 인해 프로그램의 실행 속도가 크게 향상됩니다.


---

## 7.  캐시 메모리의 확장 개념

캐시 개념은 컴퓨터 내부뿐 아니라, **네트워크와 웹 서버** 등 다양한 분야에서 활용됩니다. 예를 들어, 웹 서버에서는 자주 사용하는 파일을 미리 저장해두고 빠르게 제공하는 캐시 서버가 있으며, **CDN(Content Delivery Network)**은 주로 사용되는 콘텐츠를 사용자가 있는 지역과 가까운 서버에 미리 저장해 둡니다. 이를 통해 네트워크 속도가 느린 상황에서도 빠르게 데이터를 제공할 수 있습니다.


---

## 참고문서

- https://velog.io/@letskuku/%EC%BB%B4%ED%93%A8%ED%84%B0%EA%B5%AC%EC%A1%B0-%EC%BA%90%EC%8B%9C-%EB%A9%94%EB%AA%A8%EB%A6%AC
- https://wikidocs.net/22297 
- https://wikidocs.net/22464 
- https://wikidocs.net/65521 
- https://wikidocs.net/65522 
- https://wikidocs.net/65523 
- https://wikidocs.net/22298
- https://velog.io/@ajm0718/%EB%A9%94%EB%AA%A8%EB%A6%AC-%EA%B5%AC%EC%A1%B0
- https://iingang.github.io/posts/OS-memory/
